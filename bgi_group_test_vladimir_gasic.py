# -*- coding: utf-8 -*-
"""BGI Group test Vladimir Gasic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lIHJxurjysDY6tOTmE6JAPc8H0X5Ha89
"""

pip install kneed

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import torch
import jax
import jax.numpy as jnp
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from collections import defaultdict, Counter
import plotly.graph_objects as go
import umap
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import adjusted_rand_score
import kneed

datainit = pd.read_csv('drive/MyDrive/E14.5_E1S3_Dorsal_Midbrain_GEM_CellBin_merge.tsv', sep='\t')

print(datainit)

df=datainit.groupby(["cell", "geneID"]).agg({"x": np.mean, "y": np.mean, "MIDCounts": np.sum})

print(df)

plt.scatter(df.x, df.y)

col=df.columns
features=col.tolist()
feature=features[1:4]
print(feature)
target_1=features[1]
target_2=features[2]

X=df.loc[:, feature].values
y_1=df.loc[:, target_1].values
y_2=df.loc[:, target_2].values

sc=StandardScaler()
X=sc.fit_transform(X)
X_df=pd.DataFrame(X, columns=feature)

pca=PCA(n_components=2)
principalComp=pca.fit_transform(X)
principaldf=pd.DataFrame(data=principalComp, columns=['PC1', 'PC2'])
principaldf.head()

pca.explained_variance_ratio_

PC1_var=pca.explained_variance_[0]
PC2_var=pca.explained_variance_[1]
print(PC1_var)
print(PC2_var)

print(pca.mean_)

v=0
l=0
for r in finalDf.PC1:
  v+= r * 10000 * np.sqrt(PC1_var)

for t in finalDf.PC2:
  l+= t * 10000 * np.sqrt(PC2_var)

v=v*2000000
l=l*20000000

# plot data
plt.scatter(df.x, df.y, alpha=0.2)
plt.arrow(x=np.mean(df.x), y=np.mean(df.y), dx=v, dy=0, width=100, facecolor="red")
plt.arrow(x=np.mean(df.x), y=np.mean(df.y), dx=0, dy=l, width=100, facecolor="green")

kmeans = KMeans (
    init="random",
    n_clusters=6,
    n_init=10,
    max_iter=300,
    random_state=42
)

kmeans.fit(X)

kmeans.cluster_centers_

kmeans_wag = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,
    "random_state": 42,
    }

sse=[]
for k in range(1,11):
  kmeans = KMeans(n_clusters=k, **kmeans_wag)
  kmeans.fit(X)
  sse.append(kmeans.inertia_)

plt.style.use("fivethirtyeight")
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

kl = kneed.KneeLocator(
    range(1, 11), sse, curve="convex", direction="decreasing"
)
kl.elbow

print(kmeans.labels_)

len(kmeans.labels_)

kmeans = KMeans (
    init="random",
    n_clusters=3,
    n_init=10,
    max_iter=300,
    random_state=42
)

kmeans.fit(X)

df_final=pd.DataFrame(df)
df_final['ClusterIndex']=kmeans.labels_
df_final.head()

px.scatter(df_final, x='x', y='y', color='ClusterIndex')